{
  "title": "Final Rotated Vector Quantization Implementation",
  "model_architecture": {
    "encoder": {
      "type": "ResNet",
      "layers": [
        {"filters": 64, "kernel_size": 3, "stride": 2},
        {"filters": 128, "kernel_size": 3, "stride": 2},
        {"filters": 256, "kernel_size": 3, "stride": 2}
      ],
      "residual_blocks": 6
    },
    "vector_quantizer": {
      "codebook_size": 8192,
      "embedding_dim": 256,
      "commitment_cost": 0.25,
      "use_rotation": true,
      "use_ema_updates": true,
      "ema_decay": 0.99
    },
    "decoder": {
      "type": "CNN Transpose with Attention",
      "layers": [
        {"filters": 128, "kernel_size": 3, "stride": 2},
        {"filters": 64, "kernel_size": 3, "stride": 2},
        {"filters": 3, "kernel_size": 3, "stride": 2}
      ],
      "attention_heads": 8
    }
  },
  "rotation_mechanism": {
    "type": "Householder Transformation",
    "preserve_angle": true,
    "gradient_scaling": 1.0
  },
  "training": {
    "batch_size": 128,
    "learning_rate": 0.0002,
    "optimizer": "AdamW",
    "epochs": 300,
    "scheduler": "CosineAnnealingLR",
    "weight_decay": 0.01
  },
  "results": {
    "reconstruction_loss": 0.0098,
    "codebook_usage": 96.8,
    "perplexity": 7950.4
  }
} 